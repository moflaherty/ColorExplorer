nzv <- nearZeroVar(processedTrain)
processedTrain <- processedTrain[,-nzv]
processedTest <- processedTest[,-nzv]
ncol(processedTrain)
str(processedTest)
descrCorr <- cor(processedTrain[,sapply(processedTrain, is.numeric)]) # locate all numeric columns and coorelate
highCorr <- findCorrelation(descrCorr, 0.90)
processedTrain <- processedTrain[, -highCorr]
processedTest <- processedTest[, -highCorr]
str(processedTest)
ncol(processedTrain)
str(processedTest)
str(processedTrain)
train <- read.csv("./data/train.csv", header=TRUE)
test <- read.csv("./data/test.csv", header=TRUE)
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- train[!naRow,]
nzv <- nearZeroVar(processedTrain)
processedTrain <- processedTrain[,-nzv]
processedTest <- processedTest[,-nzv]
train <- read.csv("./data/train.csv", header=TRUE)
test <- read.csv("./data/test.csv", header=TRUE)
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- train[!naRow,]
processedTest <- test
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- train[!naRow,]
processedTest <- test                 # no NA's were found in the test data set.
nzv <- nearZeroVar(processedTrain)
processedTrain <- processedTrain[,-nzv]
processedTest <- processedTest[,-nzv]
descrCorr <- cor(processedTrain[,sapply(processedTrain, is.numeric)]) # locate all numeric columns and coorelate
highCorr <- findCorrelation(descrCorr, 0.90)
processedTrain <- processedTrain[, -highCorr]
processedTest <- processedTest[, -highCorr]
ncol(processedTrain)
str(processedTest)
train <- read.csv("./data/train.csv", header=TRUE)
test <- read.csv("./data/test.csv", header=TRUE)
# remove unnecessary columns
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- processedTrain[, !names(processedTrain) %in% colsToRemove]
processedTest <- processedTest[, !names(processedTrain) %in% colsToRemove]
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- processedTrain[, !names(train) %in% colsToRemove]
processedTest <- processedTest[, !names(test) %in% colsToRemove]
names(train)
!names(train) %in% colsToRemove
train <- read.csv("./data/train.csv", header=TRUE)
test <- read.csv("./data/test.csv", header=TRUE)
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- train[, !names(train) %in% colsToRemove]
processedTest <- test[, !names(test) %in% colsToRemove]
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- train[!naRow,]
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- processedTrain[!naRow,]
train <- read.csv("./data/train.csv", header=TRUE)
test <- read.csv("./data/test.csv", header=TRUE)
# We will be using the `caret` package exclusively the build our model.
# remove unnecessary columns
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- train[, !names(train) %in% colsToRemove]
processedTest <- test[, !names(test) %in% colsToRemove]
# Lets remove any rows that are all NA's. We start with a total count of `r format(nrow(train), big.mark=",", scientific=FALSE)` rows:
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- processedTrain[!naRow,]
stopifnot(require(caret, quietly=TRUE))
nzv <- nearZeroVar(processedTrain)
processedTrain <- processedTrain[,-nzv]
processedTest <- processedTest[,-nzv]
descrCorr <- cor(processedTrain[,sapply(processedTrain, is.numeric)]) # locate all numeric columns and coorelate
highCorr <- findCorrelation(descrCorr, 0.90)
processedTrain <- processedTrain[, -highCorr]
processedTest <- processedTest[, -highCorr]
str(processedTest)
ncol(processedTrain)
processedTrain$classe
fitRF <- train(classe~.,method='rf', data=processedTrain)
train <- read.csv("./data/train.csv", header=TRUE)
test <- read.csv("./data/test.csv", header=TRUE)
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- train[, !names(train) %in% colsToRemove]
processedTest <- test[, !names(test) %in% colsToRemove]
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- processedTrain[!naRow,]
str(processedTrain)
summary(processedTrain$amplitude_roll_dumbbell)
table(processedTrain$amplitude_roll_dumbbell)
summary(processedTrain$amplitude_roll_dumbbell)
naCol <- (colSums(is.na(train)) == 0)
naCol
naCol <- (colSums(is.na(processedTrain)) == 0)
processedTrain <- processedTrain[,!naCol]
processedTest <- processedTest[,!naCol]
str(processedTrain)
processedTrain[is.na(processedTrain)] <- 0
str(processedTrain)
summary(processedTrain$stddev_pitch_dumbbell)
table(processedTrain$stddev_pitch_dumbbell)
processedTrain
processedTest
test
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- train[, !names(train) %in% colsToRemove]
processedTest <- test[, !names(test) %in% colsToRemove]
processedTest
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- processedTrain[!naRow,]
naCol <- (colSums(is.na(processedTrain)) == 0)
naCol
processedTrain <- processedTrain[,!naCol]
processedTest <- processedTest[,!naCol]
processedTest
processedTrain
naCol
test$accel_dumbbell_x
train$accel_dumbbell_x
sum(train$accel_dumbbell_x)
sum(train$accel_dumbbell_x) == 0
naCol
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- train[, !names(train) %in% colsToRemove]
processedTest <- test[, !names(test) %in% colsToRemove]
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- processedTrain[!naRow,]
naCol <- (colSums(is.na(processedTrain)) == 0)
processedTrain <- processedTrain[,naCol]
processedTest <- processedTest[,naCol]
processedTest
processedTrain
str(processedTrain)
descrCorr <- cor(processedTrain[,sapply(processedTrain, is.numeric)]) # locate all numeric columns and coorelate
highCorr <- findCorrelation(descrCorr, 0.90)
processedTrain <- processedTrain[, -highCorr]
processedTest <- processedTest[, -highCorr]
processedTrain
str(processedTrain)
train <- read.csv("./data/train.csv", header=TRUE)
test <- read.csv("./data/test.csv", header=TRUE)
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- train[, !names(train) %in% colsToRemove]
processedTest <- test[, !names(test) %in% colsToRemove]
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- processedTrain[!naRow,]
stopifnot(require(caret, quietly=TRUE))
nzv <- nearZeroVar(processedTrain)
processedTrain <- processedTrain[,-nzv]
processedTest <- processedTest[,-nzv]
View(processedTest)
View(train)
processedTrain
str(processedTrain)
View(processedTrain)
summary(processedTrain)
str(processedTrain)
descrCorr <- cor(processedTrain[,sapply(processedTrain, is.numeric)]) # locate all numeric columns and coorelate
highCorr <- findCorrelation(descrCorr, 0.90)
processedTrain <- processedTrain[, -highCorr]
processedTest <- processedTest[, -highCorr]
View(processedTrain)
subset <- createDataPartition(y = processedTrain$classe, p=0.6, list=FALSE)
processedTrain.subset <- processedTrain[subset, ]
processedTrain.validationSet <- processedTrain[-subSet, ]
subset <- createDataPartition(y = processedTrain$classe, p=0.6, list=FALSE)
processedTrain.subset <- processedTrain[subset, ]
processedTrain.validationSet <- processedTrain[-subset, ]
fitRF <- train(classe~.,method='rf', data=processedTrain.subset)
subset <- createDataPartition(y=processedTrain$classe, p=0.9, list=FALSE)
processedTrain.subset <- processedTrain[subset, ]
processedTrain.validationSet <- processedTrain[-subset, ]
subset <- createDataPartition(y=processedTrain$classe, p=0.9, list=FALSE)
processedTrain.trainSet <- processedTrain[subset, ]
processedTrain.validationSet <- processedTrain[-subset, ]
fitRF <- train(classe~., method='rf', data=processedTrain.validationSet, trControl=trainControl(method="cv"), number=3)
install.packages("shiny")
library(shiny)
runExample("01_hello")
require(XML)
require(tm)
require(wordcloud)
require(RColorBrewer)
u = "http://cran.r-project.org/web/packages/available_packages_by_date.html"
t = readHTMLTable(u)[[1]]
ap.corpus <- Corpus(DataframeSource(data.frame(as.character(t[,3]))))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 <- brewer.pal(8,"Dark2")
png("wordcloud_packages.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
require(XML)
install.packages("XML")
install.packages("tm")
install.packages("wordcloud")
install.packages("RColorBrewer")
require(XML)
require(tm)
require(wordcloud)
require(RColorBrewer)
u = "http://cran.r-project.org/web/packages/available_packages_by_date.html"
t = readHTMLTable(u)[[1]]
ap.corpus <- Corpus(DataframeSource(data.frame(as.character(t[,3]))))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
ap.m <- as.matrix(ap.tdm)
t = readHTMLTable(u)[[1]]
ap.corpus <- Corpus(DataframeSource(data.frame(as.character(t[,3]))))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, tolower)
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.tdm <- TermDocumentMatrix(ap.corpus)
fitRF
fitGBM <- train(classe~., method='gbm', data=processedTrain.validationSet)
runExample("02_text") # tables and data frames
runExample("03_reactivity") # a reactive expression
data(SOTU)
corp <- SOTU
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, function(x)removeWords(x,stopwords()))
term.matrix <- TermDocumentMatrix(corp)
term.matrix <- as.matrix(term.matrix)
colnames(term.matrix) <- c("SOTU 2010","SOTU 2011")
comparison.cloud(term.matrix,max.words=40,random.order=FALSE)
commonality.cloud(term.matrix,max.words=40,random.order=FALSE)
corp
class(SOTU)
runExample("05_sliders") # slider bars
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project")
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project", display.mode="showcase")
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project", display.mode="showcase")
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
install.packages("googleVis")
library(googleVis)
Geo=gvisGeoMap(Map, locationvar='State', numvar='Percentage', options=list(height=350, dataMode=’regions’))
require(datasets)
states <- data.frame(state.name, state.x77)
G3 <- gvisGeoChart(states, "state.name", "Illiteracy",
options=list(region="US", displayMode="regions",
resolution="provinces",
width=600, height=400))
plot(G3)
train <- read.csv("./data/train.csv", header=TRUE)
test <- read.csv("./data/test.csv", header=TRUE)
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- train[, !names(train) %in% colsToRemove]
processedTest <- test[, !names(test) %in% colsToRemove]
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- processedTrain[!naRow,]
stopifnot(require(caret, quietly=TRUE))
nzv <- nearZeroVar(processedTrain)
processedTrain <- processedTrain[,-nzv]
processedTest <- processedTest[,-nzv]
train <- read.csv("./data/train.csv", header=TRUE)
test <- read.csv("./data/test.csv", header=TRUE)
# remove unnecessary columns
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- train[, !names(train) %in% colsToRemove]
processedTest <- test[, !names(test) %in% colsToRemove]
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- processedTrain[!naRow,]
stopifnot(require(caret, quietly=TRUE))
nzv <- nearZeroVar(processedTrain)
processedTrain <- processedTrain[,-nzv]
processedTest <- processedTest[,-nzv]
ncol(processedTrain)
descrCorr <- cor(processedTrain[,sapply(processedTrain, is.numeric)]) # locate all numeric columns and coorelate
highCorr <- findCorrelation(descrCorr, 0.90)
processedTrain <- processedTrain[, -highCorr]
processedTest <- processedTest[, -highCorr]
preProcess(processedTrain)
processedTrain
str(processedTrain)
preProcess(processedTrain[,-1])
preProcess(processedTrain[,-46])
xTrans <-preProcess(processedTrain[,-46])
predict(xTrans, processedTrain)
subset <- createDataPartition(y=processedTrain$classe, p=0.6, list=FALSE)
processedTrain.trainSet <- processedTrain[subset, ]
processedTrain.validationSet <- processedTrain[-subset, ]
?train
set.seed(777)
fitRF <- train(classe~., method='rf', data=processedTrain.trainSet, trControl=trainControl(method="cv"))
fitGBM <- train(classe~., method='gbm', data=processedTrain.trainSet)
fitRF
pred1 <- predict(fitRF, processedTrain.validationSet)
predRF <- predict(fitRF, processedTrain.validationSet)
confusionMatrix(table(predRF, processedTrain.validationSet$classe))
predRF2 <- predict(fitRF, processedTest)
confusionMatrix(table(predRF2, processedTest$classe))
predRF2 <- predict(fitRF, processedTest)
predRF2
confusionMatrix(table(predRF2, processedTest$classe))
processedTest$classe
processedTest
confusionMatrix(table(predRF, processedTrain.validationSet$classe))
library(gcookbook)
library(ggplot2)
library(grid)
library(maps)
islice <- subset(isabel, z == min(z))
ggplot(islice, aes(x=x, y=y)) + geom_segment(aes(xend=x+vx/50, yend=y+vy/50), size=0.25) # Make the line segments 0.25 mm thick
# Take a slice where z is equal to the minimum value of z
islice <- subset( isabel, z == min( z))
# Keep 1 out of every 'by' values in vector x
every_n <- function(x, by=2) {
x <- sort(x)
x[ seq( 1, length( x), by = by)] }
# Keep 1 of every 4 values in x and y
keepx <- every_n(unique(isabel $ x), by=4)
keepy <- every_n(unique(isabel $ y), by=4)
# Keep only those rows where x value is in keepx and y value is in keepy
islicesub <- subset(islice, x %in% keepx & y %in% keepy)
ggplot( islicesub, aes( x = x, y = y)) + geom_segment( aes( xend = x + vx/ 50, yend = y + vy/ 50), arrow = arrow( length = unit( 0.1, "cm")), size = 0.25)
usa <- map_data("usa")
ggplot( islicesub, aes( x = x, y = y)) + geom_segment( aes( xend = x + vx/ 50, yend = y + vy/ 50, colour = speed), arrow = arrow( length = unit( 0.1,"cm")), size = 0.6) + scale_colour_continuous( low ="grey80", high ="darkred") + geom_path( aes( x = long, y = lat, group = group), data = usa) + coord_cartesian( xlim = range( islicesub $ x), ylim = range( islicesub $ y))
n=12
pie(rep(1,n), col=FALSE)
pie(rep(1,n), col=rainbow(n))
pie(rep(1,n), col=heat.colors(n))
pie(rep(1,n), col=terrain.colors(n))
pie(rep(1,n), col=topo.colors(n))
pie(rep(1,n), col=cm.colors(n))
barplot(seq(1:12), col=topo.colors(12))
barplot(seq(1:n), col=topo.colors(n))
barplot(n)
barplot(seq(1:n))
barplot(seq(1:n), col=FALSE)
barplot(seq(1:n), col=rainbow(n))
barplot(seq(1:n), col=heat.colors(n))
barplot(seq(1:n), col=terrain.colors(n))
barplot(seq(1:n), col=topo.colors(n))
barplot(seq(1:n), col=cm.colors(n))
library(shiny)
runExample("01_hello") # a histogram
runExample("02_text") # tables and data frames
runExample("03_reactivity") # a reactive expression
runExample("04_mpg") # global variables
runExample("05_sliders") # slider bars
library(shiny)
runExample("01_hello") # a histogram
runExample("01_hello") # a histogram
runExample("02_text") # tables and data frames
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
shiny::runApp('F:/Documents/eInvasion/School/Coursera/John Hopkins University/Data Science/9 - Developing Data Products/Course Project')
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runExample("02_text") # tables and data frames
source('F:/Documents/eInvasion/School/Coursera/John Hopkins University/Data Science/9 - Developing Data Products/Course Project.R', echo=TRUE)
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runExample("02_text") # tables and data frames
runExample("01_hello") # a histogram
train <- read.csv("./data/train.csv", header=TRUE)
test <- read.csv("./data/test.csv", header=TRUE)
colsToRemove <- c("X", "user_name", "raw_timestamp_part_1", "new_window",
"num_window", "raw_timestamp_part_2", "cvtd_timestamp")
processedTrain <- train[, !names(train) %in% colsToRemove]
processedTest <- test[, !names(test) %in% colsToRemove]
# Lets remove any rows that are all NA's. We start with a total count of `r format(nrow(train), big.mark=",", scientific=FALSE)` rows:
naRow <- (rowSums(is.na(train)) == 0) # locate rows that are all NA's
processedTrain <- processedTrain[!naRow,]
stopifnot(require(caret, quietly=TRUE))
nzv <- nearZeroVar(processedTrain)
processedTrain <- processedTrain[,-nzv]
processedTest <- processedTest[,-nzv]
descrCorr <- cor(processedTrain[,sapply(processedTrain, is.numeric)]) # locate all numeric columns and coorelate
highCorr <- findCorrelation(descrCorr, 0.90)
processedTrain <- processedTrain[, -highCorr]
processedTest <- processedTest[, -highCorr]
subset <- createDataPartition(y=processedTrain$classe, p=0.6, list=FALSE)
processedTrain.trainSet <- processedTrain[subset, ]
processedTrain.validationSet <- processedTrain[-subset, ]
fitRF <- train(classe~., method='rf', data=processedTrain.trainSet, trControl=trainControl(method="cv"))
fitRF
install.packages("broom")
library(broom)
predVS <- predict(fitRF, processedTrain.validationSet)
confusionMatrix(table(predVS, processedTrain.validationSet$classe))
confusionMatrix(table(predVS, processedTrain.validationSet$classe))$Accuracy
confusionMatrix(table(predVS, processedTrain.validationSet$classe))$accuracy
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[1,1]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[1]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[2]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[3]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[4]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[5]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[6]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[7]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[4]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[3]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[3][0]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[3][1]
source('~/.active-rstudio-document', echo=TRUE)
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[3][2]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))[3]
confusionMatrix(table(predVS, processedTrain.validationSet$classe))$overall
predTest <- predict(fitRF, processedTest)
predTest
library(shiny)
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runExample("01_hello") # a histogram
runExample("07_widgets") # help text and submit buttons
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
install.packages("shiny")
runExample("01_hello") # a histogram
library(shiny)
runExample("01_hello") # a histogram
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runExample("02_text") # tables and data frames
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runExample("01_hello") # a histogram
runExample("01_hello") # a histogram
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
pie(rep(1,n), col=FALSE)
n=12
pie(rep(1,n), col=FALSE)
pie(rep(1,n), col=rainbow(n))
pie(rep(1,n), col=heat.colors(n))
pie(rep(1,n), col=terrain.colors(n))
pie(rep(1,n), col=topo.colors(n))
pie(rep(1,n), col=cm.colors(n))
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runExample("01_hello") # a histogram
runExample("02_text") # tables and data frames
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
install.packages("shinyapps")
install.packages("shinyapps")
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
install.packages("shinyapps")
library(shinyapps)
shinyapps::setAccountInfo(name='moflaherty', token='6AD1B07788634D107CD9E704F4F841E3', secret='aPGO4XTNqvsbYm5i6AKVKKHkIwp/ucHy/1WiobbF')
deployApp()
library(shinyapps)
shinyapps::setAccountInfo(name='moflaherty', token='6AD1B07788634D107CD9E704F4F841E3', secret='aPGO4XTNqvsbYm5i6AKVKKHkIwp/ucHy/1WiobbF')
deployApp()
deployApp()
library(shiny)
runApp()
setwd("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project")
deployApp()
shinyapps::setAccountInfo(name='moflaherty', token='6AD1B07788634D107CD9E704F4F841E3', secret='aPGO4XTNqvsbYm5i6AKVKKHkIwp/ucHy/1WiobbF')
deployApp()
library(shinyapps)
deployApp()
shinyapps::setAccountInfo(name='ColorExplorer', token='6AD1B07788634D107CD9E704F4F841E3', secret='aPGO4XTNqvsbYm5i6AKVKKHkIwp/ucHy/1WiobbF')
moflaherty
shinyapps::setAccountInfo(name='moflaherty', token='6AD1B07788634D107CD9E704F4F841E3', secret='aPGO4XTNqvsbYm5i6AKVKKHkIwp/ucHy/1WiobbF')
runApp("F:\\Documents\\eInvasion\\School\\Coursera\\John Hopkins University\\Data Science\\9 - Developing Data Products\\Course Project") # , display.mode="showcase"
